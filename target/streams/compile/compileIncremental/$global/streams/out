[0m[[0mdebug[0m] [0m[naha] [0m
[0m[[0mdebug[0m] [0m[naha] Initial source changes: [0m
[0m[[0mdebug[0m] [0m[naha] 	removed:Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	added: Set(H:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala, H:\documentation\projects\nlp\src\main\scala\com\spark\nlp\util\NGram.scala)[0m
[0m[[0mdebug[0m] [0m[naha] 	modified: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Invalidated products: Set()[0m
[0m[[0mdebug[0m] [0m[naha] External API changes: API Changes: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Modified binary dependencies: Set()[0m
[0m[[0mdebug[0m] [0m[naha] Initial directly invalidated sources: Set(H:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala, H:\documentation\projects\nlp\src\main\scala\com\spark\nlp\util\NGram.scala)[0m
[0m[[0mdebug[0m] [0m[naha] [0m
[0m[[0mdebug[0m] [0m[naha] Sources indirectly invalidated by:[0m
[0m[[0mdebug[0m] [0m[naha] 	product: Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	binary dep: Set()[0m
[0m[[0mdebug[0m] [0m[naha] 	external source: Set()[0m
[0m[[0mdebug[0m] [0mAll initially invalidated sources: Set(H:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala, H:\documentation\projects\nlp\src\main\scala\com\spark\nlp\util\NGram.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Initial set of included nodes: Set(H:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala, H:\documentation\projects\nlp\src\main\scala\com\spark\nlp\util\NGram.scala)[0m
[0m[[0mdebug[0m] [0m[naha] Recompiling all 2 sources: invalidated sources (2) exceeded 50.0% of all sources[0m
[0m[[0minfo[0m] [0mCompiling 2 Scala sources to H:\documentation\projects\nlp\target\scala-2.10\classes...[0m
[0m[[0mdebug[0m] [0mGetting org.scala-sbt:compiler-interface:0.13.16:component from component compiler for Scala 2.10.6[0m
[0m[[0mdebug[0m] [0mGetting org.scala-sbt:compiler-interface:0.13.16:component from component compiler for Scala 2.10.6[0m
[0m[[0mdebug[0m] [0mRunning cached compiler 67368814, interfacing (CompilerInterface) with Scala compiler version 2.10.6[0m
[0m[[0mdebug[0m] [0mCalling Scala compiler with arguments  (CompilerInterface):[0m
[0m[[0mdebug[0m] [0m	-bootclasspath[0m
[0m[[0mdebug[0m] [0m	G:\Software\java1.8\jre\lib\resources.jar;G:\Software\java1.8\jre\lib\rt.jar;G:\Software\java1.8\jre\lib\sunrsasign.jar;G:\Software\java1.8\jre\lib\jsse.jar;G:\Software\java1.8\jre\lib\jce.jar;G:\Software\java1.8\jre\lib\charsets.jar;G:\Software\java1.8\jre\lib\jfr.jar;G:\Software\java1.8\jre\classes;C:\Users\Yafang\.sbt\boot\scala-2.10.6\lib\scala-library.jar[0m
[0m[[0mdebug[0m] [0m	-classpath[0m
[0m[[0mdebug[0m] [0m	H:\documentation\projects\nlp\target\scala-2.10\classes;C:\Users\Yafang\.ivy2\cache\org.ansj\ansj_seg\jars\ansj_seg-5.1.6.jar;C:\Users\Yafang\.ivy2\cache\org.nlpcn\nlp-lang\jars\nlp-lang-1.7.7.jar[0m
[0m[[33mwarn[0m] [0mH:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala:2: imported `util' is permanently hidden by definition of package util in package nlp[0m
[0m[[33mwarn[0m] [0mimport java.util[0m
[0m[[33mwarn[0m] [0m            ^[0m
[0m[[31merror[0m] [0mH:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala:4: object apache is not a member of package org[0m
[0m[[31merror[0m] [0mimport org.apache.spark._[0m
[0m[[31merror[0m] [0m           ^[0m
[0m[[31merror[0m] [0mH:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala:5: not found: object SparkContext[0m
[0m[[31merror[0m] [0mimport SparkContext._[0m
[0m[[31merror[0m] [0m       ^[0m
[0m[[31merror[0m] [0mH:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala:6: object jersey is not a member of package com.sun[0m
[0m[[31merror[0m] [0mimport com.sun.jersey.core.impl.provider.entity.XMLJAXBElementProvider.Text[0m
[0m[[31merror[0m] [0m               ^[0m
[0m[[31merror[0m] [0mH:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala:10: object apache is not a member of package org[0m
[0m[[31merror[0m] [0mimport org.apache.log4j.{Level, Logger}[0m
[0m[[31merror[0m] [0m           ^[0m
[0m[[31merror[0m] [0mH:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala:12: object apache is not a member of package org[0m
[0m[[31merror[0m] [0mimport org.apache.avro.mapred.SequenceFileInputFormat[0m
[0m[[31merror[0m] [0m           ^[0m
[0m[[31merror[0m] [0mH:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala:15: not found: value Logger[0m
[0m[[31merror[0m] [0m    Logger.getLogger("org.apache.spark").setLevel(Level.OFF)[0m
[0m[[31merror[0m] [0m    ^[0m
[0m[[31merror[0m] [0mH:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala:21: not found: type SparkConf[0m
[0m[[31merror[0m] [0m    val conf = new SparkConf().setAppName("WordCount").setMaster("local")//ÊåáÂÆöÁ®ãÂ∫èÂêçÁß∞‰∏∫wordcountÔºåmaster‰∏∫Êú¨Âú∞[0m
[0m[[31merror[0m] [0m                   ^[0m
[0m[[31merror[0m] [0mH:\documentation\projects\nlp\src\main\scala\com\spark\nlp\Boot.scala:22: not found: type SparkContext[0m
[0m[[31merror[0m] [0m    val sc = new SparkContext(conf)[0m
[0m[[31merror[0m] [0m                 ^[0m
[0m[[33mwarn[0m] [0mone warning found[0m
[0m[[31merror[0m] [0m8 errors found[0m
[0m[[0mdebug[0m] [0mCompilation failed (CompilerInterface)[0m
[0m[[31merror[0m] [0m(compile:[31mcompileIncremental[0m) Compilation failed[0m
